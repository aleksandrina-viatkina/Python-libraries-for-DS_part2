{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cheap-requirement",
   "metadata": {},
   "source": [
    "**Задание 1.** Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-algebra",
   "metadata": {},
   "source": [
    "Усреднение применяется в случаях, когда мы хотим получить среднюю метрику по всем классам, например если классов много, или мы хотим увидеть картину в целом.\n",
    "\n",
    "**Микроусреднение** - учитывает только верные срабатывания модели. Определяется как отношение верных срабатываний к общему числу объектов выборки. Для каждого класса она не расчитывается. Часто заменяют на accuracy\n",
    "\n",
    "**Макроусреднение** - считает каждый класс одинаково важным. Считается метрика относительно каждого из классов, после чего они усредняются средним арифметическим. Макроусреднее особенно актуально, когда сильный дисбаланс классов, а мы хотим, чтобы все классы были важны одинаково. При такой цели используя взвешенное среднее, класс с малым числом объектом \"потеряется\" в расчете.\n",
    "\n",
    "**Взвешенное усреднение (weighted)** - идея состоит в расчете аналогично взвешенному среднему. Т.е., считается доля каждого класса в выборке [кол-во объектов класса/всего объектов выборки] и умножается на метрику, рассчитанную относительно этого класса. Так делается с каждым классом и все вместе суммируется. То есть, класс с бОльшим количеством объектов в выборке будет иметь больший вес. Имеет смысл, если нас интересует наибольший класс."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-character",
   "metadata": {},
   "source": [
    "**Задание 2.** В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-surfing",
   "metadata": {},
   "source": [
    "**XGBoost**\n",
    "\n",
    "Экстремальный вариант Градиентного бустинга, способный обработать датасет быстро и качественно. В отличии от других алгоритмов с деревьями (где рассчитывается энтропия или критерий Джинни), рассчитывает \"похожесть\" (сумма отклонений в квадрате/(предыдущая вероятность*(1-предыдущая предсказанная вероятность + коэфф регуляризации))\n",
    "\n",
    "Также с помощью параметра гамма (повышая его) можно снижать переобучение и \"стричь деревья\", а также регулировать скорость обучения с помощью параметра learning rate. То есть можно регулировать переобучение.\n",
    "\n",
    "**LightBoost**\n",
    "Модель обучается быстрее других бустингов.  Это достигается за счет того, что эта модель при обучении использует выборку неполностью. Используются методы:\n",
    "\n",
    "1) GOSS - lightboost сохраняет объекты с большими градиентами(большими ошибками) и случайным образом выполняет выборку среи объектов с маленькими градиентами. Основная идея здесь -  выборки с обучающими экземплярами с небольшими градиентами имеют меньшую ошибку обучения, и она уже хорошо обучена.\n",
    "Чтобы сохранить то же распределение данных, при вычислении прироста информации GOSS вводит постоянный множитель для экземпляров данных с небольшими градиентами. Таким образом, GOSS обеспечивает хороший баланс между уменьшением количества экземпляров данных и сохранением точности для выученных деревьев решений.\n",
    "\n",
    "2)EFB, идея которого заключается в том, чтобы сократить объем разряженных данных, допускается объединение признаков с разными диапазонами уникальных значений в один общий признак. Допустим как в данном случае - значения, которые принимает признак а-(10,20), б - (30,40). \n",
    "\n",
    "Еще одной особенностью является построение дерева по листочкам, то есть дерево строится в глубину.\n",
    "\n",
    "**CatBoost**\n",
    "\n",
    "Наиболее интересная и отличающаяся от остальных модель, созданная сотрудниками Яндекс.\n",
    "Основные особенности: \n",
    "* дерево симметрично (\"вопросы\" дублируются в соседние ветки симметрично), \n",
    "* на каждой итерации обучается несколько деревьев, после чего ошибка считается на объектах, на которых модель не обучалась и которых она не видела (то есть ошибка получается объективнее, за счет чего снижается переобучения\n",
    "* постоянное перемешивание выборки, что улучшает качество обучение модели - высокий уровень рандома\n",
    "* работа с категориальными признаками, нет необходимости их кодировать или переводить в дамми"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-receptor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-snapshot",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
